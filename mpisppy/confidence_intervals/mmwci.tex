\documentclass[11pt]{article}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{url}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.1in}
\setlength{\textheight}{8.5in}
\setlength{\textwidth}{5.9in} %.7
\setlength{\topmargin}{-.48in}
\setlength{\oddsidemargin}{.1in} %.29
\renewcommand{\thebibliography}[1]{\vspace{2\baselineskip}\noindent
           {\large\bf Bibliography}\small
           \nopagebreak\vspace{.5\baselineskip}\nopagebreak
           \list{[\arabic{enumi}]}{\settowidth{\labelwidth}{[#1]}
           \leftmargin\labelwidth \advance\leftmargin\labelsep
           \usecounter{enumi}}
           \def\newblock{\hskip .11em plus .33em minus -.07em}
           \sloppy \sfcode`\.=1000\relax}

\title {MMW Confidence interval construction}


%\author{David L.\ Woodruff \\
%Graduate School of Management\\
%University of California Davis\\
%Davis CA 95616, USA\\
%Email: dlwoodruff@ucdavis.edu
%}

\begin{document}

\section{Introduction}

This is a brief write up of the construction of the confidence interval on the optimality gap of a candidate solution $\mathbf{\hat{x}}$ in a stochastic program. This is directly related to \textit{mmw\_ci.py} and the stand alone program \textit{mmw\_conf.py} in the \textit{confidence\_intervals} module of \textit{mpisppy}.

\section{MMW confidence interval}
For a given candidate solution $\mathbf{\hat{x}}$ to the stochastic program
$$
	\text{SP} \quad z^* = \min_{x\in X} E f(\mathbf{x}, \mathbf{\tilde{\xi}}) \text{ with } \mathbf{x^*} \in \underset{x \in X}{\text{argmin }}Ef(\mathbf{x}, \mathbf{\tilde{\xi}}),
$$
Mak, Morton and Wood construct a $(1 - \alpha)$-level confidence interval on the optimality gap at $\mathbf{\hat{x}}$ of the form
$$
	[0, \bar{G}(n_g) + \tilde{\epsilon}_g],
$$
where 
$$
 \bar{G}(n_g) = n_g^{-1} \sum_{j=1}^{n_g} G_n^j,
$$
with 
$$
G_n^j = E\left[\frac{1}{n} \sum_{i=1}^n f(\hat{\mathbf{x}}, \mathbf{\tilde{\xi}}^{ji})  - \min_{x\in X}\sum_{i=1}^n f(\mathbf{x}, \mathbf{\tilde{\xi}}^{ji}) \right].
$$

This relies on the inequality in (9), which gives that this expectation is bounded below by $Ef(\mathbf{\hat{x}}, \mathbf{\tilde{\xi}}) - z^*.$

Each batch $j = 1, \dots, n_g$ corresponds to a realization of the optimality gap (with batch size $n$), and we take the average of these to construct the confidence interval. We have
$$
\sqrt{n_g} [\bar{G}(n_g) - EG_n] \Rightarrow \text{N}(0, \sigma_g^2) \text{ as } n_g \to \infty,
$$
so, with $s_g(n_g)$ denoting the sample variance estimator of $\sigma_g^2,$ we define
$$
	\tilde{\epsilon}_g = \frac{t_{n_g - 1, \alpha} s_g(n_g)}{\sqrt{n_g}}
$$
which satisfies the inequality
$$
P\left(E f(\mathbf{\hat{x}}, \mathbf{\tilde{\xi}}) - z^* \le \bar{G}(n_g) + \tilde{\epsilon}_g\right) \ge 1 - \alpha.
$$

\subsection{Confidence interval around $Ef(\mathbf{\hat{x}}, \mathbf{\tilde{\xi}})$}
The issue here is that in practice we cannot explicitly calculate $Ef(\mathbf{\hat{x}}, \mathbf{\tilde{\xi}}).$ To construct the optimality gap around the value of the objective function at $\mathbf{\hat{x}},$ we construct an additional confidence interval to ensure 
$$
P\left(\left|Ef(\mathbf{\hat{x}}, \mathbf{\tilde{\xi}}) - (n_g n)^{-1} \sum_{i, j} f(\mathbf{\hat{x}}, \mathbf{\tilde{\xi}}^{ji})\right| \le \tilde{\epsilon}_{\hat{z}}\right) \ge 1 - \alpha.
$$
where the factor $\tilde{\epsilon}_{\hat{z}}$ is given by
$$
	\tilde{\epsilon}_{\hat{z}} = \frac{t_{N-1, \alpha} s_{\hat{z}}(N)}{\sqrt{N}},
$$
with $N = n_g \cdot n.$ Here $s_{\hat{z}}(N)$ is the sample variance estimator of the the variance of the objective values $f(\mathbf{\hat{x}}, \mathbf{\tilde{\xi}}^{ji}), j = 1, \dots, n_g, i = 1, \dots, n.$ Thus we have a $(1-\alpha)$-level confidence interval around $Ef(\mathbf{\hat{x}}, \mathbf{\tilde{\xi}})$ given by
$$
\left[(n_g \cdot n)^{-1} \sum_{i, j} f(\mathbf{\hat{x}}, \mathbf{\tilde{\xi}}^{ji}) - \tilde{\epsilon}_{\hat{z}}, (n_g \cdot n)^{-1} \sum_{i, j} f(\mathbf{\hat{x}}, \mathbf{\tilde{\xi}}^{ji}) + \bar{G}(n_g) + \tilde{\epsilon}_g + \tilde{\epsilon}_{\hat{z}}\right].
$$

\end{document}
